<h1 style="text-align:center;">
	<br />
</h1>
<h1 style="text-align:center;">
	SELF-SUPERVISED LEARNING FOR AUDIO-VISUAL SPEAKER DIARIZATION
</h1>
<p>
	<br />
</p>
<div style="text-align:center;">
	<a href="mailto:yf.ding@knights.ucf.edu?subject=SELF-SUPERVISED LEARNING FOR AUDIO-VISUAL SPEAKER DIARIZATION">Yifan Ding</a><sup>1</sup>, <a href="mailto:lucayongxu@tencent.com?subject=SELF-SUPERVISED LEARNING FOR AUDIO-VISUAL SPEAKER DIARIZATION">Yong Xu</a><sup>2</sup>, <a href="mailto:auszhang@tencent.com?subject=SELF-SUPERVISED LEARNING FOR AUDIO-VISUAL SPEAKER DIARIZATION">Shi-Xiong Zhang</a> <sup>2</sup>, <a href="mailto:congyahuan@bupt.edu.cn?subject=SELF-SUPERVISED LEARNING FOR AUDIO-VISUAL SPEAKER DIARIZATION">Yahuan Cong</a> <sup>3</sup> and <a href="mailto:lwang@cs.ucf.edu?subject=SELF-SUPERVISED LEARNING FOR AUDIO-VISUAL SPEAKER DIARIZATION">Liqiang Wang</a><sup>1</sup> 
</div>
<p style="text-align:center;">
	<sup>1</sup>University of Central Florida, USA, <sup>2</sup>Tencent AI Lab, USA, <sup>3</sup>BUPT, China
</p>
<p style="text-align:center;">
	<br />
</p>
<h4 style="text-align:center;">
	<a href="#1">Abstract</a>&nbsp;&nbsp; |&nbsp; &nbsp;<a href="#2">Network Architecture</a>&nbsp;&nbsp; |&nbsp; &nbsp;<a href="#3">Training Examples</a>&nbsp;&nbsp; |&nbsp;&nbsp; <a href="#4">Losses</a>&nbsp;&nbsp; |&nbsp;&nbsp; <a href="#5">More Test Results</a>&nbsp;&nbsp; |&nbsp;&nbsp; <a href="#6">Per-frame Test Distances</a>&nbsp;&nbsp; |&nbsp; &nbsp;<a href="#7">Video Demos</a>
</h4>
<p style="text-align:center;">
	<br />
</p>
<p style="text-align:center;">
	<table style="width:90%;" cellpadding="2" cellspacing="20" border="0" bordercolor="#000000" class="ke-zeroborder" align="center">
		<tbody>
			<tr>
				<td>
					<h2>
						Abstract<a name="1"></a> 
					</h2>
					<p>
						Speaker diarization, which is to find the speech segments of specific speakers, has been widely used in human-centered applications such as video conferences or human-computer interaction systems.&nbsp; &nbsp;In this paper, we propose a self-supervised audio-video synchronization learning method to address the problem of speaker diarization without massive labeling effort. &nbsp; We &nbsp;improve the previous approaches by introducing two new loss functions: &nbsp;the dynamic triplet loss and the multinomial loss. &nbsp;We test them on a real-world human-computer interaction system and the results show our best model yields a remarkable gain of +10% (F1-scores) and-12% (DER). Finally, we introduce a new large scale audio-video corpus designed to fill the vacancy of the audio-video datasets in Chinese.
					</p>
				</td>
			</tr>
		</tbody>
	</table>
<br class="Apple-interchange-newline" />
<br />
	<table style="width:90%;" cellpadding="2" cellspacing="20" border="0" bordercolor="#000000" class="ke-zeroborder" align="center">
		<tbody>
			<tr>
				<td style="vertical-align:top;width:50%;">
					<h2 class="font_2" style="vertical-align:baseline;font-weight:normal;font-size:24px;font-family:arial, &quot;color:#2C2C2B;">
						<span style="vertical-align:baseline;font-weight:bold;"><span style="font-size:18px;">Network </span><span style="font-size:18px;">Architecture<a name="2"></a></span></span> 
					</h2>
					<p style="text-align:center;">
						<img src="./files/network6.jpg" width="90%" alt="" /> 
					</p>
					<p style="text-align:center;">
						<br />
					</p>
					<p style="text-align:center;">
						Fig. 1. Two stream network architecture.
					</p>
					<p style="text-align:center;">
						<br />
					</p>
					<p style="text-align:justify;">
						We process audio and video separately using a two-stream network, &nbsp;a common approach for multi-modal tasks. &nbsp;For the audio stream, the input is transformed with DCT (Discrete cosine transform) and MFCC(Mel &nbsp;Frequency &nbsp;Cepstral &nbsp;Coefficient) &nbsp;feature,i.e., &nbsp;a&nbsp; power spectrum of a sound on a non-linear mel scale of frequency. Then the MFCC is sent to a 2D convolutional network to produce speech features.&nbsp; &nbsp;For the video stream, &nbsp;a &nbsp;3D &nbsp;convolution module is employed to extract both temporal information between consecutive video frames and spatial information in each video frame.&nbsp;
						
					</p>
				</td>
				<td style="vertical-align:top;width:50%;">
					<h2>
						<span style="font-size:18px;">Training Examples<a name="3"></a></span> 
					</h2>
					<p style="text-align:center;">
						<img src="./files/samples6.jpg" width="90%" alt="" /> 
					</p>
					<p style="text-align:center;">
						<br />
					</p>
					<p style="text-align:center;">
						Fig. 2. Examples of audio-video synchronized, shifted, and&nbsp;heterologous pairs.&nbsp;W&nbsp;denotes the length of visual clip.&nbsp;T&nbsp;is&nbsp;the shifting range.
					</p>
					<p style="text-align:center;">
						<br />
					</p>
					<p style="text-align:justify;">
						We consider three types of training samples in our experiments:&nbsp;the synchronized pair (positive) in which the audio and video are synchronized, the&nbsp;shifted pair (negative) with different shifting range, in which the audio and video are from the same source but different time clip, the heterologous pair (negative) in which the audio and video are from different sources.
					</p>
					<p>
						<br />
					</p>
				</td>
			</tr>
		</tbody>
	</table>
</p>
<p>
	<table style="width:90%;" cellpadding="2" cellspacing="20" border="0" class="ke-zeroborder" bordercolor="#000000" align="center">
		<tbody>
			<tr>
				<td>
					<h2>
						<span style="font-size:18px;">Losses<a name="4"></a></span><span style="font-size:18px;"></span> 
					</h2>
					<p style="text-align:center;">
						<span style="font-size:18px;"><img src="./files/losses8.jpg" width="90%" alt="" /><br />
</span> 
					</p>
					<p style="text-align:left;">
						<span><span><br />
</span></span> 
					</p>
					<p style="text-align:left;">
						<span><span>Fig. 3. Comparison of loss functions. (a):&nbsp;The negative pair is separated by a margin.&nbsp;(b):&nbsp;Negative pairs are separated from&nbsp;each other by a margin.&nbsp; (c):&nbsp; Negative pairs with different shifts or heterologous audio are separated by separate margins.</span><span></span><br />
</span> 
					</p>
					<p style="text-align:left;">
						<span><span><br />
</span></span> 
					</p>
					<p style="text-align:justify;">
						<span><span>We propose two new losses: the dynamic triplet loss and multinomial loss. In the dynamic triplet loss, positive pairs&nbsp;and negatives pairs are dynamically determined in each iteration.&nbsp;In the multinomial loss,&nbsp;all shifting combinations for an audio-video pair and all heterologous combinations for audio-video pairs within a mini-batch are considered simultaneously.&nbsp;Specifically, we cluster the negative pairs into groups, where different margins with LogSumExp &nbsp;(LSE) are employed to achieve a smooth maximum in each group. The experiment results show that multinomial loss achieves faster convergence and better performance compared with dynamic triplet loss and contrastive loss.<br />
</span></span> 
					</p>
				</td>
			</tr>
		</tbody>
	</table>
</p>
<p>
	<br />
	<table style="width:90%;" cellpadding="2" cellspacing="20" align="center" border="0" class="ke-zeroborder" bordercolor="#000000">
		<tbody>
			<tr>
				<td style="vertical-align:top;width: 50%">
					<h2>
						<span style="font-size:18px;">More Test&nbsp;Results<a name="5"></a></span><span style="font-size:18px;"></span> 
					</h2>
					<p style="text-align:center;">
						<img src="./files/results.png" width="90%" alt="" /> 
					</p>
					<p>
						<br />
					</p>
					<p style="text-align:justify;">
						We compare our results with SyncNet and &nbsp;UIS-RNN&nbsp; (unbounded interleaved-state recurrent neural networks) with/without faces as unbound.&nbsp; &nbsp;UIS-RNN is a fully-supervised audio-only speaker diarization system that takes d-vector embedding as input and each individual speaker is modeled by a parameter-sharing RNN, while the RNN states for different speakers interleave in the time domain. &nbsp;For the face bounded version, the number of faces is used to indicate the number of speakers, which has video involved.
					</p>
				</td>
				<td style="vertical-align:top;">
					<h2>
						<span style="font-size:18px;">Per-frame Test Distances<a name="6"></a></span><span style="font-size:18px;"></span> 
					</h2>
					<p>
						<span style="font-size:18px;"><br />
</span> 
					</p>
					<p style="text-align:center;">
						<img src="./files/dist3.jpg" width="90%" alt="" /> 
					</p>
					<p>
						<br />
					</p>
					<p>
						Fig. 4. (a): Ours. Different colors denote the audio-video&nbsp;distance of different speakers. The curve with the lowest distance is the predicted active speaker. (b): SyncNet. (c): GT. (d): Visualization of GT. The red box frames the face of active speakers.
					</p>
				</td>
			</tr>
		</tbody>
	</table>
</p>
<p>
	<table style="width:90%;" cellpadding="2" cellspacing="20" align="center" border="0" class="ke-zeroborder" bordercolor="#000000">
		<tbody>
			<tr>
				<td>
					<span id="__kindeditor_bookmark_start_388__"></span> 
					<h2>
						<span style="font-size:18px;"></span><span style="font-size:18px;">Video Demo<a name="7"></span>
					</h2>
				</td>
				<td>
					<br />
				</td>
			</tr>
			<tr>
				<td style="vertical-align:top;">
					<p>
						GT label
					</p>
					<p style="text-align:center;">
						<video width="90%" controls><source src="./files/video_out_labeled.mp4" type="video/mp4"></video>
					</p>
				
				</td>
				<td style="vertical-align:top;">
					<p>
						ours
					</p>
					<p style="text-align:center;">
						<video width="90%" controls><source src="./files/video_out_ours.mp4" type="video/mp4"></video>
						
					</p>
				
				</td>
			</tr>
			<tr>
				<td>
					The green boxes denote the labeled/detected active speaker.<br />
				</td>
				<td>
					<br />
				</td>
			</tr>
		</tbody>
	</table>
</p>